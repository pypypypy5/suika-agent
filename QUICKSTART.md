# ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

ìˆ˜ë°•ê²Œì„ ê°•í™”í•™ìŠµ í”„ë¡œì íŠ¸ë¥¼ ë¹ ë¥´ê²Œ ì‹œì‘í•˜ê¸° ìœ„í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## âš¡ í•µì‹¬ ë³€ê²½ì‚¬í•­

**ì¤‘ìš”:** ì´ë¯¸ì§€ ê¸°ë°˜ í™˜ê²½ ëŒ€ì‹  **ìƒíƒœ ê¸°ë°˜ í™˜ê²½**ì„ ì‚¬ìš©í•˜ì„¸ìš”!

- âœ… 1000ë°° íš¨ìœ¨ì 
- âœ… CNN ë¶ˆí•„ìš”, MLPë§Œìœ¼ë¡œ OK
- âœ… 100ë°° ë¹ ë¥¸ í•™ìŠµ
- âœ… ëª…í™•í•œ ë””ë²„ê¹…

```python
# â­ ì¶”ì²œ: ìƒíƒœ ê¸°ë°˜
from envs import SuikaStateWrapper
env = SuikaStateWrapper()
obs = np.ndarray(62,)  # 62ê°œ ê°’

# í˜¸í™˜ì„±: ì´ë¯¸ì§€ ê¸°ë°˜
from envs import SuikaEnvWrapper
env = SuikaEnvWrapper()
obs = {'image': (400,300,3), 'score': float}
```

## 1. í™˜ê²½ ì„¤ì • (5ë¶„)

### ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python -m venv venv
source venv/bin/activate
```

### ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

### Suika RL í™˜ê²½ ì„¤ì¹˜
```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ
git clone https://github.com/edwhu/suika_rl.git
cd suika_rl
pip install -e .
cd ..
```

## 2. ì˜ˆì œ ì‹¤í–‰ (1ë¶„)

í™˜ê²½ì´ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸:
```bash
python example_usage.py
```

ì´ ì˜ˆì œëŠ” ë‹¤ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤:
- í™˜ê²½ê³¼ì˜ ê¸°ë³¸ ìƒí˜¸ì‘ìš©
- ì—ì´ì „íŠ¸ ì‚¬ìš©ë²•
- ì»¤ìŠ¤í…€ ë³´ìƒ í•¨ìˆ˜
- í†µê³„ ì •ë³´ í™œìš©

## 3. ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ êµ¬í˜„

### 3.1 ì—ì´ì „íŠ¸ íŒŒì¼ ìƒì„±

`agents/my_agent.py` íŒŒì¼ì„ ë§Œë“¤ê³  ë‹¤ìŒ í…œí”Œë¦¿ì„ ì‚¬ìš©:

```python
import torch
import torch.nn as nn
from agents.base_agent import RLAgent

class MyMLPAgent(RLAgent):
    """ìƒíƒœ ê¸°ë°˜ í™˜ê²½ìš© MLP ì—ì´ì „íŠ¸"""

    def __init__(self, observation_space, action_space, config=None):
        super().__init__(observation_space, action_space, config)

        # MLP ë„¤íŠ¸ì›Œí¬ (CNN í•„ìš” ì—†ìŒ!)
        obs_dim = observation_space.shape[0]  # 62
        self.policy_net = nn.Sequential(
            nn.Linear(obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()  # 0~1 ë²”ìœ„ í–‰ë™
        ).to(self.device)

        self.optimizer = torch.optim.Adam(
            self.policy_net.parameters(),
            lr=self.learning_rate
        )

    def _forward_policy(self, obs, deterministic):
        # ê°„ë‹¨í•œ forward pass
        return self.policy_net(obs)

    def update(self, obs, action, reward, next_obs, done):
        # í•™ìŠµ ë¡œì§ (DQN, PPO ë“±)
        # return {'loss': loss_value}
        pass
```

### 3.2 main.pyì— ì—ì´ì „íŠ¸ ë“±ë¡

`main.py`ì˜ `create_agent` í•¨ìˆ˜ì— ì¶”ê°€:

```python
elif agent_type == 'my_agent':
    from agents.my_agent import MyAgent
    agent = MyAgent(
        observation_space=env.observation_space,
        action_space=env.action_space,
        config=agent_config
    )
```

### 3.3 ì„¤ì • íŒŒì¼ ìˆ˜ì •

`config/default.yaml`ì—ì„œ ì—ì´ì „íŠ¸ íƒ€ì… ë³€ê²½:

```yaml
agent:
  type: "my_agent"  # 'random'ì—ì„œ 'my_agent'ë¡œ ë³€ê²½
```

## 4. í•™ìŠµ ì‹œì‘

```bash
python main.py --mode train --config config/default.yaml
```

í•™ìŠµ ì§„í–‰ ìƒí™©ì€ ë‹¤ìŒì—ì„œ í™•ì¸:
- ì½˜ì†” ì¶œë ¥
- TensorBoard: `tensorboard --logdir experiments/tensorboard`
- ì²´í¬í¬ì¸íŠ¸: `experiments/checkpoints/`

## 5. ëª¨ë¸ í‰ê°€

```bash
python main.py --mode eval --checkpoint experiments/checkpoints/best_model.pth
```

## ì¶”ì²œ í•™ìŠµ ìˆœì„œ

### Phase 1: í™˜ê²½ ì´í•´
1. `example_usage.py` ì‹¤í–‰í•˜ì—¬ í™˜ê²½ íŒŒì•…
2. `envs/suika_wrapper.py` ì½”ë“œ ì½ê¸°
3. ê´€ì°° ê³µê°„ê³¼ í–‰ë™ ê³µê°„ ì´í•´

### Phase 2: ë² ì´ìŠ¤ë¼ì¸ ì„¤ì •
1. Random Agentë¡œ í•™ìŠµ ì‹¤í–‰
2. ì„±ëŠ¥ ê¸°ë¡ (í‰ê·  ë³´ìƒ ë“±)
3. ì´ê²ƒì´ ê°œì„  ëª©í‘œ!

### Phase 3: ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
1. ê°„ë‹¨í•œ ì•Œê³ ë¦¬ì¦˜ë¶€í„° ì‹œì‘ (DQN ì¶”ì²œ)
2. `agents/base_agent.py`ì˜ `RLAgent` ìƒì†
3. `_forward_policy`ì™€ `update` ë©”ì„œë“œ êµ¬í˜„

### Phase 4: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
1. `config/default.yaml`ì—ì„œ íŒŒë¼ë¯¸í„° ì¡°ì •
2. í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì‹¤í—˜
3. WandBë¡œ ì‹¤í—˜ ì¶”ì  (ì„ íƒì‚¬í•­)

## í”„ë¡œì íŠ¸ êµ¬ì¡° ìš”ì•½

```
melon-ai/
â”œâ”€â”€ envs/              # í™˜ê²½ ë˜í¼ (ìˆ˜ì • ë¶ˆí•„ìš”)
â”‚   â””â”€â”€ suika_wrapper.py
â”œâ”€â”€ agents/            # ì—¬ê¸°ì— ì—ì´ì „íŠ¸ êµ¬í˜„!
â”‚   â”œâ”€â”€ base_agent.py  # ìƒì†ë°›ì„ ë² ì´ìŠ¤ í´ë˜ìŠ¤
â”‚   â””â”€â”€ your_agent.py  # êµ¬í˜„í•  ì—ì´ì „íŠ¸
â”œâ”€â”€ models/            # ì‹ ê²½ë§ ëª¨ë¸ (ì„ íƒì‚¬í•­)
â”œâ”€â”€ training/          # í•™ìŠµ ë£¨í”„ (ìˆ˜ì • ë¶ˆí•„ìš”)
â”œâ”€â”€ utils/             # ìœ í‹¸ë¦¬í‹° (ìˆ˜ì • ë¶ˆí•„ìš”)
â”œâ”€â”€ config/            # ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ default.yaml   # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
â””â”€â”€ main.py           # ì‹¤í–‰ íŒŒì¼
```

## í•µì‹¬ ì¸í„°í˜ì´ìŠ¤

ì—ì´ì „íŠ¸ êµ¬í˜„ ì‹œ í•„ìˆ˜ ë©”ì„œë“œ:

```python
class YourAgent(RLAgent):
    def _forward_policy(self, obs, deterministic):
        """ê´€ì°° -> í–‰ë™"""

    def update(self, obs, action, reward, next_obs, done):
        """ê²½í—˜ìœ¼ë¡œë¶€í„° í•™ìŠµ"""

    def save(self, path):
        """ëª¨ë¸ ì €ì¥"""

    def load(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
```

í™˜ê²½ ì¸í„°í˜ì´ìŠ¤ëŠ” í‘œì¤€ Gymnasium:
- `env.reset()` â†’ observation, info
- `env.step(action)` â†’ observation, reward, terminated, truncated, info

## ë¬¸ì œ í•´ê²°

### Suika RL í™˜ê²½ì´ ì—†ëŠ” ê²½ìš°
- Mock í™˜ê²½ì´ ìë™ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤
- ì‹¤ì œ í™˜ê²½ ì„¤ì¹˜ëŠ” ì„¹ì…˜ 1 ì°¸ê³ 

### CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±
- `config/default.yaml`ì—ì„œ `batch_size` ì¤„ì´ê¸°
- ë„¤íŠ¸ì›Œí¬ í¬ê¸° ì¶•ì†Œ

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦° ê²½ìš°
- `system.num_workers` ì¦ê°€ (ë³‘ë ¬ í™˜ê²½)
- í‰ê°€ ë¹ˆë„ ì¤„ì´ê¸° (`training.eval_freq`)

## ë‹¤ìŒ ë‹¨ê³„

1. DQN êµ¬í˜„ ì˜ˆì œ ì°¾ê¸°
2. PPO, SAC ë“± ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ì‹œë„
3. ì»¤ìŠ¤í…€ ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„
4. ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ ê°œì„ 

í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸ‰
