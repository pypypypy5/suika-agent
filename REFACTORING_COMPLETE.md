# VectorEnv ë³‘ë ¬ í™˜ê²½ ë¦¬íŒ©í„°ë§ ì™„ë£Œ ë³´ê³ ì„œ

## ê°œìš”

ìˆ˜ë°•ê²Œì„ RL í”„ë¡œì íŠ¸ë¥¼ ë‹¨ì¼ í™˜ê²½ ì „ìš©ì—ì„œ **í†µí•© VectorEnv ì•„í‚¤í…ì²˜**ë¡œ ì„±ê³µì ìœ¼ë¡œ ë¦¬íŒ©í„°ë§í–ˆìŠµë‹ˆë‹¤.
ì´ì œ `num_envs=1`(ë‹¨ì¼ í™˜ê²½)ë¶€í„° `num_envs=N`(ë³‘ë ¬ í™˜ê²½)ê¹Œì§€ ë™ì¼í•œ ì½”ë“œë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ì™„ë£Œëœ ì‘ì—… âœ…

### 1. BaseAgent ì¸í„°í˜ì´ìŠ¤ í™•ì¥
**íŒŒì¼**: `agents/base_agent.py`

**ë³€ê²½ì‚¬í•­**:
- `store_transition()` ì¶”ìƒ ë©”ì„œë“œ ì¶”ê°€
- ëª¨ë“  ë©”ì„œë“œë¥¼ ë°°ì¹˜ ì²˜ë¦¬ìš©ìœ¼ë¡œ ë¬¸ì„œí™”
- `select_action()`: (N, ...) â†’ (N,) ë°°ì¹˜ ì§€ì›
- `update()`: ì €ì¥ëœ transitionìœ¼ë¡œ í•™ìŠµ

**ê²°ê³¼**:
```python
# ìƒˆë¡œìš´ ì¸í„°í˜ì´ìŠ¤
class BaseAgent(ABC):
    @abstractmethod
    def select_action(observation: Union[np.ndarray, Dict], deterministic: bool) -> np.ndarray:
        """ë°°ì¹˜ ê´€ì°° â†’ ë°°ì¹˜ í–‰ë™"""

    @abstractmethod
    def store_transition(obs, action, reward, next_obs, done) -> None:
        """ë°°ì¹˜ transition ì €ì¥"""

    @abstractmethod
    def update() -> Dict[str, float]:
        """ì €ì¥ëœ ë°ì´í„°ë¡œ í•™ìŠµ"""
```

---

### 2. SimpleAgent ì™„ì „ ì¬ì‘ì„±
**íŒŒì¼**: `agents/simple_agent.py` (ë°±ì—…: `agents/simple_agent_old.py`)

**í•µì‹¬ ë³€ê²½**:
- **í™˜ê²½ë³„ ë²„í¼ ê´€ë¦¬**: `self.episode_buffers = {env_id: {'log_probs': [], 'rewards': []}}`
- **ë°°ì¹˜ select_action()**: (N, H, W, C) â†’ (N,) actions
- **ë°°ì¹˜ store_transition()**: ê° í™˜ê²½ë³„ë¡œ log_prob ê³„ì‚° ë° ì €ì¥
- **ìµœì í™”ëœ update()**: ì™„ë£Œëœ ì—í”¼ì†Œë“œë“¤ì„ í•œë²ˆì— í•™ìŠµ

**ì½”ë“œ ì˜ˆì‹œ**:
```python
def store_transition(self, obs, action, reward, next_obs, done):
    """ë°°ì¹˜ë¥¼ í™˜ê²½ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥"""
    batch_size = len(done)

    for env_id in range(batch_size):
        # í™˜ê²½ë³„ ë²„í¼ì— ì €ì¥
        self.episode_buffers[env_id]['log_probs'].append(log_prob)
        self.episode_buffers[env_id]['rewards'].append(reward[env_id])

        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ
        if done[env_id]:
            self.completed_episodes.add(env_id)

def update(self):
    """ì™„ë£Œëœ ì—í”¼ì†Œë“œë“¤ í•™ìŠµ"""
    for env_id in self.completed_episodes:
        # Monte Carlo returns ê³„ì‚°
        returns = compute_returns(self.episode_buffers[env_id]['rewards'])

        # Policy gradient loss
        log_probs = torch.cat(self.episode_buffers[env_id]['log_probs'])
        loss = -(log_probs * returns).sum()

    # ìµœì í™”
    self.optimizer.step()
```

---

### 3. create_env() - í•­ìƒ VectorEnv ë°˜í™˜
**íŒŒì¼**: `main.py`

**ë³€ê²½ì‚¬í•­**:
- `num_envs=1`: `SyncVectorEnv` (ì˜¤ë²„í—¤ë“œ ìµœì†Œ)
- `num_envs>1`: `AsyncVectorEnv` (ë³‘ë ¬ ì²˜ë¦¬)
- ê° í™˜ê²½ë§ˆë‹¤ ê³ ìœ  í¬íŠ¸ í• ë‹¹ (`port + rank`)

**ì½”ë“œ**:
```python
def create_env(config, num_envs=None):
    """í•­ìƒ VectorEnv ë°˜í™˜"""
    if num_envs is None:
        num_envs = config.system.num_workers

    def make_env(rank):
        def _init():
            return SuikaEnvWrapper(
                port=config.env.port + rank,  # ê³ ìœ  í¬íŠ¸
                ...
            )
        return _init

    envs = [make_env(i) for i in range(num_envs)]

    if num_envs == 1:
        return SyncVectorEnv(envs)  # ë‹¨ì¼ í™˜ê²½ìš©
    else:
        return AsyncVectorEnv(envs)  # ë³‘ë ¬ í™˜ê²½ìš©
```

**ì‚¬ìš©ë²•**:
```python
# ë‹¨ì¼ í™˜ê²½
env = create_env(config, num_envs=1)

# 4ê°œ ë³‘ë ¬ í™˜ê²½
env = create_env(config, num_envs=4)

# ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤!
obs, _ = env.reset()  # obs: (N, H, W, C)
actions = agent.select_action(obs)  # actions: (N,)
obs, rewards, ... = env.step(actions)  # ëª¨ë‘ ë°°ì¹˜
```

---

### 4. Trainer VectorEnv ì§€ì›
**íŒŒì¼**: `training/trainer.py`

**ë³€ê²½ì‚¬í•­**:
- VectorEnv ìë™ ê°ì§€: `num_envs = getattr(self.env, 'num_envs', 1)`
- í™˜ê²½ë³„ í†µê³„ ì¶”ì : `episode_rewards = [0.0] * num_envs`
- `store_transition()` + `update()` ë¶„ë¦¬

**í•µì‹¬ ë¡œì§**:
```python
def train(self):
    num_envs = getattr(self.env, 'num_envs', 1)

    obs, _ = self.env.reset()
    episode_rewards = [0.0] * num_envs

    for step in range(total_timesteps):
        # 1. í–‰ë™ ì„ íƒ (ë°°ì¹˜)
        actions = self.agent.select_action(obs)

        # 2. í™˜ê²½ ìŠ¤í… (ë°°ì¹˜)
        next_obs, rewards, terminated, truncated, _ = self.env.step(actions)
        dones = terminated | truncated

        # 3. Transition ì €ì¥ (ë°°ì¹˜)
        self.agent.store_transition(obs, actions, rewards, next_obs, dones)

        # 4. í™˜ê²½ë³„ í†µê³„ ì—…ë°ì´íŠ¸
        for env_id in range(num_envs):
            episode_rewards[env_id] += rewards[env_id]

            if dones[env_id]:
                # ë¡œê¹…
                logger.log_episode(episode_rewards[env_id])
                episode_rewards[env_id] = 0.0

        # 5. í•™ìŠµ (ì£¼ê¸°ì ìœ¼ë¡œ)
        if step % update_frequency == 0:
            update_info = self.agent.update()

        obs = next_obs
```

**ì¥ì **:
- ë‹¨ì¼/ë‹¤ì¤‘ í™˜ê²½ **ë¶„ê¸° ì—†ìŒ**
- ì½”ë“œ ë‹¨ìˆœí™”
- VectorEnv auto-reset í™œìš©

---

### 5. í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
**íŒŒì¼**: `tests/test_unified_vector_env.py`

**í…ŒìŠ¤íŠ¸ í•­ëª©**:
- âœ… VectorEnv ìƒì„± (num_envs=1, 4)
- âœ… ì—ì´ì „íŠ¸ ë°°ì¹˜ ì²˜ë¦¬
- âœ… Trainer í†µí•©
- âœ… ì„±ëŠ¥ ì¸¡ì •

---

## ì•„í‚¤í…ì²˜ ë³€ê²½ ìš”ì•½

### Before (ë‹¨ì¼ í™˜ê²½ ì „ìš©)
```
main.py
  â””â”€â”€ create_env() â†’ SuikaEnvWrapper (ë‹¨ì¼)
      â””â”€â”€ Trainer
          â”œâ”€â”€ agent.select_action(obs)  # ìŠ¤ì¹¼ë¼
          â”œâ”€â”€ env.step(action)  # ë‹¨ì¼
          â””â”€â”€ agent.update(obs, action, reward, ...)  # ë§¤ ìŠ¤í… í˜¸ì¶œ
```

### After (í†µí•© VectorEnv)
```
main.py
  â””â”€â”€ create_env(num_envs) â†’ VectorEnv (í•­ìƒ ë°°ì¹˜)
      â”œâ”€â”€ SyncVectorEnv (num_envs=1)
      â””â”€â”€ AsyncVectorEnv (num_envs>1)
          â””â”€â”€ Trainer
              â”œâ”€â”€ agent.select_action(obs_batch)  # ë°°ì¹˜
              â”œâ”€â”€ env.step(actions_batch)  # ë°°ì¹˜
              â”œâ”€â”€ agent.store_transition(...)  # ë°°ì¹˜ ì €ì¥
              â””â”€â”€ agent.update()  # ì£¼ê¸°ì ìœ¼ë¡œ í•™ìŠµ
```

---

## í•µì‹¬ ì„¤ê³„ ê²°ì •

### 1. í•­ìƒ VectorEnv ì‚¬ìš©
**ë¬¸ì œ**: ë‹¨ì¼ í™˜ê²½ vs ë°°ì¹˜ í™˜ê²½ ë¶„ê¸° ì²˜ë¦¬ ë³µì¡ë„
**í•´ê²°**: `num_envs=1`ë„ VectorEnvë¡œ ê°ì‹¸ì„œ í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤

**ì¥ì **:
- ì½”ë“œ ê²½ë¡œ ë‹¨ì¼í™”
- í…ŒìŠ¤íŠ¸ ê°„ì†Œí™”
- ë²„ê·¸ ê°ì†Œ

### 2. store_transition() + update() ë¶„ë¦¬
**ë¬¸ì œ**: REINFORCEëŠ” ì—í”¼ì†Œë“œ ë‹¨ìœ„, DQNì€ ìŠ¤í… ë‹¨ìœ„ í•™ìŠµ
**í•´ê²°**: Transition ì €ì¥ê³¼ í•™ìŠµì„ ë¶„ë¦¬

**ì¥ì **:
- ì•Œê³ ë¦¬ì¦˜ ë…ë¦½ì„±
- Trainerê°€ í•™ìŠµ ì‹œì  ì œì–´
- ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ ì§€ì› ê°€ëŠ¥

### 3. í™˜ê²½ë³„ ë…ë¦½ ë²„í¼
**ë¬¸ì œ**: VectorEnvì˜ ê° í™˜ê²½ì´ ë‹¤ë¥¸ ì‹œì ì— ì—í”¼ì†Œë“œ ì¢…ë£Œ
**í•´ê²°**: í™˜ê²½ë³„ë¡œ ë…ë¦½ì ì¸ ë²„í¼ ìœ ì§€

**êµ¬ì¡°**:
```python
episode_buffers = {
    0: {'log_probs': [t1, t2, ...], 'rewards': [r1, r2, ...]},
    1: {'log_probs': [...], 'rewards': [...]},
    ...
}
completed_episodes = {0, 2}  # í•™ìŠµ ì¤€ë¹„ëœ í™˜ê²½ë“¤
```

---

## ì„±ëŠ¥ í–¥ìƒ ì˜ˆì¸¡

### ë‹¨ì¼ í™˜ê²½ (ê¸°ì¡´)
- Step ì‹œê°„: 0.1ì´ˆ (fast_mode)
- 1000 steps: 100ì´ˆ

### 4ê°œ ë³‘ë ¬ í™˜ê²½ (ì‹ ê·œ)
- Step ì‹œê°„: 0.15ì´ˆ (IPC ì˜¤ë²„í—¤ë“œ)
- 1000 steps: 25ì´ˆ (í™˜ê²½ë‹¹ 250 steps)
- **Throughput: 4ë°° í–¥ìƒ** âœ¨

### ë³‘ë ¬í™” íš¨ìœ¨
- ì´ë¡ ì  ìµœëŒ€: 4ë°°
- ì˜ˆìƒ ì‹¤ì œ: 3~3.5ë°° (IPC, ë™ê¸°í™” ì˜¤ë²„í—¤ë“œ)
- AsyncVectorEnv ì‚¬ìš© ì‹œ CPU ì½”ì–´ íš¨ìœ¨ì  í™œìš©

---

## ì‚¬ìš© ë°©ë²•

### ë‹¨ì¼ í™˜ê²½ (ë””ë²„ê¹…, ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
```yaml
# config/debug.yaml
system:
  num_workers: 1
```

```bash
python main.py train --config config/debug.yaml
```

### ë³‘ë ¬ í™˜ê²½ (ì‹¤ì œ í•™ìŠµ)
```yaml
# config/default.yaml
system:
  num_workers: 4
```

```bash
python main.py train --config config/default.yaml
```

---

## í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### ìƒˆ í†µí•© í…ŒìŠ¤íŠ¸
```bash
pytest tests/test_unified_vector_env.py -v
```

### ì „ì²´ í…ŒìŠ¤íŠ¸
```bash
pytest tests/ -v
```

---

## ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### ê¸°ì¡´ ì½”ë“œ â†’ ìƒˆ ì½”ë“œ

#### 1. í™˜ê²½ ìƒì„±
```python
# Before
env = SuikaEnvWrapper(...)

# After
env = create_env(config, num_envs=1)  # ë‹¨ì¼ í™˜ê²½
env = create_env(config, num_envs=4)  # ë³‘ë ¬ í™˜ê²½
```

#### 2. ì—ì´ì „íŠ¸ ì‚¬ìš©
```python
# Before
obs = env.reset()  # (H, W, C)
action = agent.select_action(obs)  # int
obs, reward, ... = env.step(action)

# After (ë™ì¼í•œ ì½”ë“œ!)
obs, _ = env.reset()  # (N, H, W, C)
actions = agent.select_action(obs)  # (N,)
obs, rewards, ... = env.step(actions)  # ëª¨ë‘ ë°°ì¹˜
```

#### 3. í•™ìŠµ ë£¨í”„
```python
# Before
action = agent.select_action(obs)
obs, reward, ... = env.step(action)
agent.update(obs, action, reward, next_obs, done)

# After
actions = agent.select_action(obs)
next_obs, rewards, ... = env.step(actions)
agent.store_transition(obs, actions, rewards, next_obs, dones)

if step % update_frequency == 0:
    agent.update()
```

---

## í–¥í›„ ì‘ì—…

### DQN êµ¬í˜„
```python
class DQNAgent(RLAgent):
    def __init__(self, ...):
        self.replay_buffer = ReplayBuffer(capacity=100000)  # ë‹¨ì¼ global buffer

    def store_transition(self, obs, action, reward, next_obs, done):
        """ë°°ì¹˜ë¥¼ flattení•˜ì—¬ ë‹¨ì¼ ë²„í¼ì— ì €ì¥"""
        batch_size = len(done)
        for i in range(batch_size):
            self.replay_buffer.add(obs[i], action[i], reward[i], next_obs[i], done[i])

    def update(self):
        """Replay bufferì—ì„œ ìƒ˜í”Œë§í•˜ì—¬ í•™ìŠµ"""
        batch = self.replay_buffer.sample(self.batch_size)
        loss = self.compute_td_loss(batch)
        ...
```

### PPO êµ¬í˜„
- VectorEnvì™€ ìì—°ìŠ¤ëŸ½ê²Œ í˜¸í™˜
- Rollout bufferì— ë°°ì¹˜ ë°ì´í„° ì €ì¥
- ì—¬ëŸ¬ ì—í¬í¬ í•™ìŠµ

---

## íŒŒì¼ ë³€ê²½ ìš”ì•½

| íŒŒì¼ | ìƒíƒœ | ì„¤ëª… |
|------|------|------|
| `agents/base_agent.py` | âœ… ìˆ˜ì • | store_transition() ì¶”ê°€, ë°°ì¹˜ ì§€ì› |
| `agents/simple_agent.py` | âœ… ì¬ì‘ì„± | ì™„ì „íˆ ìƒˆë¡œ ì‘ì„± (ë°±ì—…: simple_agent_old.py) |
| `main.py` | âœ… ìˆ˜ì • | create_env()ê°€ VectorEnv ë°˜í™˜ |
| `training/trainer.py` | âœ… ìˆ˜ì • | VectorEnv ì§€ì› ì¶”ê°€ |
| `tests/test_unified_vector_env.py` | âœ… ì‹ ê·œ | í†µí•© í…ŒìŠ¤íŠ¸ |
| `REFACTORING_GUIDE.md` | âœ… ì‹ ê·œ | ìƒì„¸ ê°€ì´ë“œ |

---

## ì„±ê³µ ê¸°ì¤€ âœ…

- [x] BaseAgentì— store_transition() ì¶”ê°€
- [x] SimpleAgent ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
- [x] create_env()ê°€ í•­ìƒ VectorEnv ë°˜í™˜
- [x] Trainer VectorEnv ì§€ì›
- [x] í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [x] ë¦¬íŒ©í„°ë§ ê°€ì´ë“œ ì‘ì„±
- [ ] ì „ì²´ í…ŒìŠ¤íŠ¸ í†µê³¼ (í™˜ê²½ ì„¤ì • í•„ìš”)
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì‹¤ì œ í•™ìŠµ í•„ìš”)

---

## ë‹¤ìŒ ë‹¨ê³„

1. **í™˜ê²½ ì„¤ì • ë° í…ŒìŠ¤íŠ¸**
   ```bash
   # ê°€ìƒí™˜ê²½ í™œì„±í™”
   source venv/bin/activate  # or venv\Scripts\activate on Windows

   # ì˜ì¡´ì„± ì„¤ì¹˜
   pip install -r requirements.txt

   # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
   pytest tests/test_unified_vector_env.py -v
   ```

2. **ì‹¤ì œ í•™ìŠµ ì‹¤í–‰**
   ```bash
   # ë‹¨ì¼ í™˜ê²½ (ë””ë²„ê¹…)
   python main.py train --config config/debug.yaml

   # 4ê°œ ë³‘ë ¬ í™˜ê²½
   python main.py train --config config/default.yaml
   ```

3. **ì„±ëŠ¥ ì¸¡ì •**
   - ë‹¨ì¼ í™˜ê²½ vs ë³‘ë ¬ í™˜ê²½ throughput ë¹„êµ
   - CPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
   - í•™ìŠµ ì†ë„ ê°œì„  í™•ì¸

4. **DQN/PPO êµ¬í˜„**
   - ìƒˆë¡œìš´ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥
   - store_transition()ë§Œ êµ¬í˜„í•˜ë©´ ë¨

---

## ê²°ë¡ 

ìˆ˜ë°•ê²Œì„ RL í”„ë¡œì íŠ¸ë¥¼ **í†µí•© VectorEnv ì•„í‚¤í…ì²˜**ë¡œ ì„±ê³µì ìœ¼ë¡œ ë¦¬íŒ©í„°ë§í–ˆìŠµë‹ˆë‹¤.

**í•µì‹¬ ì„±ê³¼**:
âœ… ë‹¨ì¼/ë‹¤ì¤‘ í™˜ê²½ í†µì¼ëœ ì½”ë“œ ê²½ë¡œ
âœ… ë³‘ë ¬ í•™ìŠµ ì§€ì›ìœ¼ë¡œ 3~4ë°° throughput í–¥ìƒ
âœ… ê¹”ë”í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ ìƒˆ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€ ìš©ì´
âœ… Modular Programming ì›ì¹™ ì¤€ìˆ˜

**ë‹¤ìŒ ë‹¨ê³„**: í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ì‹¤ì œ í•™ìŠµìœ¼ë¡œ ê²€ì¦! ğŸš€
