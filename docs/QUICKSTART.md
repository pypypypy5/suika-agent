# ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

ìˆ˜ë°•ê²Œì„ ê°•í™”í•™ìŠµ í”„ë¡œì íŠ¸ë¥¼ ë¹ ë¥´ê²Œ ì‹œì‘í•˜ê¸° ìœ„í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## âš¡ í™˜ê²½ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” **ì´ë¯¸ì§€ ê¸°ë°˜ í™˜ê²½**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
from envs import SuikaEnvWrapper

env = SuikaEnvWrapper()
obs, info = env.reset()
# obs = {'image': (400, 300, 3), 'score': float}
```

**í•„ìš”í•œ ëª¨ë¸:** CNN (Convolutional Neural Network)

## 1. í™˜ê²½ ì„¤ì • (5ë¶„)

### ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python -m venv venv
source venv/bin/activate
```

### ì˜ì¡´ì„± ì„¤ì¹˜
```bash
pip install -r requirements.txt
```

**ì£¼ì˜**: `suika_rl`ì€ ì´ë¯¸ í”„ë¡œì íŠ¸ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë³„ë„ ì„¤ì¹˜ ë¶ˆí•„ìš”!

## 2. ì˜ˆì œ ì‹¤í–‰ (1ë¶„)

í™˜ê²½ì´ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸:
```bash
python example_usage.py
```

ë˜ëŠ” í…ŒìŠ¤íŠ¸ ì‹¤í–‰:
```bash
python tests/test_simple.py
```

## 3. ì²« ë²ˆì§¸ CNN ì—ì´ì „íŠ¸ êµ¬í˜„

### 3.1 ì—ì´ì „íŠ¸ íŒŒì¼ ìƒì„±

`agents/cnn_agent.py` íŒŒì¼ì„ ë§Œë“¤ê³  ë‹¤ìŒ í…œí”Œë¦¿ì„ ì‚¬ìš©:

```python
import torch
import torch.nn as nn
from agents.base_agent import RLAgent

class CNNAgent(RLAgent):
    """ì´ë¯¸ì§€ ê¸°ë°˜ í™˜ê²½ìš© CNN ì—ì´ì „íŠ¸"""

    def __init__(self, observation_space, action_space, config=None):
        super().__init__(observation_space, action_space, config)

        # CNN ë„¤íŠ¸ì›Œí¬ (ì´ë¯¸ì§€ ì²˜ë¦¬)
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )

        # Fully connected layers
        self.fc = nn.Sequential(
            nn.Linear(64 * 46 * 34, 512),
            nn.ReLU(),
            nn.Linear(512, 1),
            nn.Sigmoid()  # 0~1 ë²”ìœ„ í–‰ë™
        )

        self.policy_net = nn.Sequential(self.conv, nn.Flatten(), self.fc).to(self.device)

        self.optimizer = torch.optim.Adam(
            self.policy_net.parameters(),
            lr=self.learning_rate
        )

    def _forward_policy(self, obs, deterministic):
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° forward pass
        return self.policy_net(obs)

    def update(self, obs, action, reward, next_obs, done):
        # í•™ìŠµ ë¡œì§ (DQN, Rainbow ë“±)
        # return {'loss': loss_value}
        pass
```

### 3.2 main.pyì— ì—ì´ì „íŠ¸ ë“±ë¡

`main.py`ì˜ `create_agent` í•¨ìˆ˜ì— ì¶”ê°€:

```python
elif agent_type == 'cnn':
    from agents.cnn_agent import CNNAgent
    agent = CNNAgent(
        observation_space=env.observation_space,
        action_space=env.action_space,
        config=agent_config
    )
```

### 3.3 ì„¤ì • íŒŒì¼ ìˆ˜ì •

`config/default.yaml`ì—ì„œ ì—ì´ì „íŠ¸ íƒ€ì… ë³€ê²½:

```yaml
agent:
  type: "cnn"  # 'random'ì—ì„œ 'cnn'ë¡œ ë³€ê²½
```

## 4. í•™ìŠµ ì‹œì‘

```bash
python main.py --mode train --config config/default.yaml
```

í•™ìŠµ ì§„í–‰ ìƒí™©ì€ ë‹¤ìŒì—ì„œ í™•ì¸:
- ì½˜ì†” ì¶œë ¥
- TensorBoard: `tensorboard --logdir experiments/tensorboard`
- ì²´í¬í¬ì¸íŠ¸: `experiments/checkpoints/`

## 5. ëª¨ë¸ í‰ê°€

```bash
python main.py --mode eval --checkpoint experiments/checkpoints/best_model.pth
```

## ê¶Œì¥ ì•Œê³ ë¦¬ì¦˜

ì´ë¯¸ì§€ ê¸°ë°˜ í™˜ê²½ì— ì í•©í•œ ì•Œê³ ë¦¬ì¦˜:

### 1. DQN (Deep Q-Network)
- CNNìœ¼ë¡œ ì´ë¯¸ì§€ ì²˜ë¦¬
- Q-learning + Experience Replay
- êµ¬í˜„ ë‚œì´ë„: â­â­â­

### 2. Rainbow DQN
- DQN + 6ê°€ì§€ ê°œì„ ì‚¬í•­
- ìµœì‹  ê¸°ë²• ì§‘í•©
- êµ¬í˜„ ë‚œì´ë„: â­â­â­â­

### 3. PPO (with CNN)
- Policy Gradient ê¸°ë°˜
- ì•ˆì •ì ì¸ í•™ìŠµ
- êµ¬í˜„ ë‚œì´ë„: â­â­â­â­

### 4. Stable Baselines3 ì‚¬ìš©
```python
from stable_baselines3 import DQN
from stable_baselines3.common.vec_env import DummyVecEnv

env = DummyVecEnv([lambda: SuikaEnvWrapper()])
model = DQN('CnnPolicy', env, verbose=1)
model.learn(total_timesteps=100000)
```

## í”„ë¡œì íŠ¸ êµ¬ì¡° ìš”ì•½

```
melon-ai/
â”œâ”€â”€ envs/              # í™˜ê²½ ë˜í¼
â”‚   â””â”€â”€ suika_wrapper.py  # ì´ë¯¸ì§€ ê¸°ë°˜
â”œâ”€â”€ agents/            # ì—¬ê¸°ì— CNN ì—ì´ì „íŠ¸ êµ¬í˜„!
â”‚   â”œâ”€â”€ base_agent.py
â”‚   â””â”€â”€ cnn_agent.py  # êµ¬í˜„í•  íŒŒì¼
â”œâ”€â”€ models/            # CNN ì•„í‚¤í…ì²˜ (ì„ íƒì‚¬í•­)
â”œâ”€â”€ training/          # í•™ìŠµ ë£¨í”„
â”œâ”€â”€ config/            # ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ default.yaml   # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
â””â”€â”€ main.py           # ì‹¤í–‰ íŒŒì¼
```

## ë¬¸ì œ í•´ê²°

### Chrome/Chromium ì„¤ì¹˜
```bash
# Ubuntu/Debian
sudo apt-get install chromium-browser chromium-chromedriver

# macOS
brew install chromedriver
```

### CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±
- `config/default.yaml`ì—ì„œ `batch_size` ì¤„ì´ê¸°
- ì´ë¯¸ì§€ í¬ê¸° ì¶•ì†Œ

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦° ê²½ìš°
- GPU ì‚¬ìš© í™•ì¸
- ë°°ì¹˜ í¬ê¸° ì¦ê°€
- ë³‘ë ¬ í™˜ê²½ ì‚¬ìš© (`system.num_workers`)

## ë‹¤ìŒ ë‹¨ê³„

1. **CNN êµ¬ì¡° ê°œì„ **: ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬, Residual connection ë“±
2. **ì•Œê³ ë¦¬ì¦˜ ì ìš©**: DQN, Rainbow, PPO ë“±
3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸° ë“±

í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸ‰
